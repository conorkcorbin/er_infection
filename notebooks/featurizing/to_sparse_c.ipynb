{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/sw/open/anaconda/3/lib/python3.6/site-packages/google/auth/_default.py:66: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os \n",
    "from google.cloud import bigquery\n",
    "from tqdm import tqdm\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/ccorbin/.config/gcloud/application_default_credentials.json' \n",
    "os.environ['GCLOUD_PROJECT'] = 'som-nero-phi-jonc101' \n",
    "\n",
    "client=bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROUND=\"validation\"\n",
    "#ROUND=\"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bq_to_pandas(query, nrows, chunksize=500000):\n",
    "    offsets = [i for i in range(0, nrows, chunksize)]\n",
    "    df = pd.DataFrame()\n",
    "    for offset in tqdm(offsets):\n",
    "        query_str = query + \" LIMIT {chunksize} OFFSET {offset}\"\n",
    "        query_str = query_str.format(chunksize=chunksize, offset=offset)\n",
    "        query_job = client.query(query_str)\n",
    "        df_slice = query_job.result().to_dataframe()\n",
    "        df = pd.concat([df, df_slice])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [10:48<00:00, 40.55s/it]\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT f.*, EXTRACT(YEAR from f.index_time) year\n",
    "FROM mining-clinical-decisions.abx.feature_counts_long f\n",
    "RIGHT JOIN  mining-clinical-decisions.abx.final_cohort_table c\n",
    "USING (pat_enc_csn_id_coded)\n",
    "ORDER BY pat_enc_csn_id_coded, features\n",
    "\"\"\"\n",
    "df = read_bq_to_pandas(query, nrows=7754289, chunksize=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sanity check - no duplicate rows\n",
    "assert(len(df) == len(df.drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation\n"
     ]
    }
   ],
   "source": [
    "if ROUND == \"validation\":\n",
    "    fold_years = [2009 + i for i in range(10)]\n",
    "    end_year = 2019\n",
    "else:\n",
    "    fold_years = [2009 + i for i in range(11)]\n",
    "    end_year = 2020\n",
    "\n",
    "print(ROUND)\n",
    "folds = []\n",
    "for year in fold_years:\n",
    "    training_examples = df[(df['year'] < end_year) & (df['year'] != year)]\n",
    "    test_examples = df[(df['year'] == year)]\n",
    "    folds.append({'training_examples' : training_examples, 'test_examples' : test_examples})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, save_npz\n",
    "\n",
    "def build_vocab(data):\n",
    "    \"\"\"Builds vocabulary for of terms from the data. Assigns each unique term to a monotonically increasing integer.\"\"\"\n",
    "    vocabulary = {}\n",
    "    for i, d in enumerate(data):\n",
    "        for j, term in enumerate(d):\n",
    "            vocabulary.setdefault(term, len(vocabulary))\n",
    "    return vocabulary\n",
    "\n",
    "def create_sparse_feature_matrix(train_data, apply_data):\n",
    "    \"\"\"Creates sparse matrix efficiently from long form dataframe.  We build a vocabulary\n",
    "       from the training set, then apply vocab to the apply_set\n",
    "       \n",
    "       Parameters\n",
    "       ----------\n",
    "       train_data : long form pandas DataFrame\n",
    "           Data to use to build vocabulary\n",
    "       apply_data : long form pandas DataFrame\n",
    "           Data to transform to sparse matrix for input to ML models\n",
    "    \n",
    "       Returns\n",
    "       -------\n",
    "       csr_data : scipy csr_matrix\n",
    "           Sparse matrix version of apply_data to feed into ML models. \n",
    "    \"\"\"\n",
    "    \n",
    "    train_features = train_data.groupby('pat_enc_csn_id_coded').agg({\n",
    "        'features' : lambda x: list(x),\n",
    "        'value' : lambda x: list(x)}).reset_index()\n",
    "    train_feature_names = [doc for doc in train_features.features.values]\n",
    "    train_feature_values = [doc for doc in train_features['value'].values]\n",
    "    train_csns = [csn for csn in train_features.pat_enc_csn_id_coded.values]\n",
    "    \n",
    "    apply_features = apply_data.groupby('pat_enc_csn_id_coded').agg({\n",
    "        'features' : lambda x: list(x),\n",
    "        'value' : lambda x: list(x)}).reset_index()\n",
    "    apply_features_names = [doc for doc in apply_features.features.values]\n",
    "    apply_features_values = [doc for doc in apply_features['value'].values]\n",
    "    apply_csns = [csn for csn in apply_features.pat_enc_csn_id_coded.values]\n",
    "\n",
    "    \n",
    "    vocabulary = build_vocab(train_feature_names)\n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    data = []\n",
    "    for i, d in enumerate(apply_features_names):\n",
    "        for j, term in enumerate(d):\n",
    "            if term not in vocabulary:\n",
    "                continue\n",
    "            else:\n",
    "                indices.append(vocabulary[term])\n",
    "                data.append(apply_features_values[i][j])\n",
    "            if j == 0:\n",
    "                # Add zero to data and max index in vocabulary to indices in case max feature indice isn't in apply features.\n",
    "                indices.append(len(vocabulary)-1)\n",
    "                data.append(0)\n",
    "        indptr.append(len(indices))\n",
    "    \n",
    "    csr_data = csr_matrix((data, indices, indptr), dtype=float)\n",
    "    \n",
    "    return csr_data, apply_csns, vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_data  = []\n",
    "for fold in folds:\n",
    "    train_csr, train_csns, train_vocab = create_sparse_feature_matrix(fold['training_examples'], fold['training_examples'])\n",
    "    test_csr, test_csns, test_and_val_vocab = create_sparse_feature_matrix(fold['training_examples'], fold['test_examples'])\n",
    "    \n",
    "    fold_data.append({'train_csr' : train_csr, 'train_csns' : train_csns, 'train_vocab': train_vocab,\n",
    "                       'test_csr' : test_csr, 'test_csns' : test_csns, 'test_and_val_vocab' : test_and_val_vocab})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_cohort = \"\"\"\n",
    "SELECT * \n",
    "FROM mining-clinical-decisions.abx.final_cohort_table\n",
    "ORDER BY pat_enc_csn_id_coded\n",
    "\"\"\"\n",
    "query_job = client.query(q_cohort)\n",
    "df_cohort = query_job.result().to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation\n"
     ]
    }
   ],
   "source": [
    "print(ROUND)\n",
    "fold_labels = []\n",
    "for i, year in enumerate(fold_years):\n",
    "    training_labels = df_cohort[(df_cohort['index_time'].dt.year < end_year) & (df_cohort['index_time'].dt.year != year)]\n",
    "    test_labels = df_cohort[(df_cohort['index_time'].dt.year == year)]\n",
    "    fold_labels.append({'training_labels' : training_labels, 'test_labels' : test_labels})\n",
    "\n",
    "    # Sanity checks\n",
    "    for a, b in zip(training_labels['pat_enc_csn_id_coded'].values, fold_data[i]['train_csns']):\n",
    "        assert a == b\n",
    "    for a, b in zip(test_labels['pat_enc_csn_id_coded'].values, fold_data[i]['test_csns']):\n",
    "        assert a == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, save_npz\n",
    "\n",
    "base_path = '/home/ccorbin/er_infection/data/censored_task/{}'\n",
    "\n",
    "for i, year in enumerate(fold_years):\n",
    "    path = base_path.format(year)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    # Save feature matrix\n",
    "    save_npz(os.path.join(path, 'training_examples.npz'), fold_data[i]['train_csr'])\n",
    "    save_npz(os.path.join(path, 'test_examples.npz'), fold_data[i]['test_csr'])\n",
    "\n",
    "    # Save labels\n",
    "    fold_labels[i]['training_labels'].to_csv(os.path.join(path, 'training_labels.csv'), index=None)\n",
    "    fold_labels[i]['test_labels'].to_csv(os.path.join(path, 'test_labels.csv'), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
